# Famous RL Finetuning Datasets for LLMs

This document outlines several widely-used datasets for reinforcement learning finetuning of large language models, with a focus on those suitable for our GRPO-ToT hybrid implementation.

## 1. GSM8K (Grade School Math 8K)

**Description**: GSM8K is a dataset of 8.5K high-quality grade school math word problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ - * /).

**Why it's suitable for GRPO-ToT**:
- Requires multi-step reasoning, perfect for demonstrating ToT's branching exploration
- Has verifiable answers, making it ideal for RLVR (Reinforcement Learning with Verifiable Rewards)
- Widely used benchmark for evaluating reasoning capabilities in LLMs
- The step-by-step nature of solutions aligns well with tree-based exploration

**Source**: [GSM8K on HuggingFace](https://huggingface.co/datasets/gsm8k)

## 2. HumanEval

**Description**: HumanEval is a dataset of 164 programming problems with unit tests. Each problem includes a function signature, docstring, body, and several unit tests. The dataset is designed to evaluate the functional correctness of code generated by language models.

**Why it's suitable for GRPO-ToT**:
- Code generation requires planning and structured thinking
- Has verifiable solutions through test cases (perfect for RLVR)
- Widely used in evaluating coding capabilities of LLMs
- Different solution approaches can be explored via ToT branching

**Source**: [HumanEval on GitHub](https://github.com/openai/human-eval)

## 3. MATH

**Description**: MATH is a dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a step-by-step solution which can be used to teach models to generate answer derivations and explanations.

**Why it's suitable for GRPO-ToT**:
- Requires advanced multi-step reasoning
- Perfect for demonstrating the expanded reasoning boundary of ToT
- Solutions can be verified mathematically
- Diverse problem types allow for testing generalization

**Source**: [MATH Dataset Paper](https://arxiv.org/abs/2103.03874)

## 4. Anthropic's Helpful and Harmless (HH-RLHF)

**Description**: A dataset of human preferences over AI assistant responses, containing both helpful and harmless preferences. It includes 160K human preference judgments between pairs of responses.

**Why it's suitable for GRPO-ToT**:
- Standard dataset for RLHF training
- Contains diverse dialogue scenarios
- Allows for exploring different reasoning paths for the same query
- Can demonstrate how ToT improves response quality

**Source**: [Anthropic's HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)

## 5. MBPP (Mostly Basic Python Programming)

**Description**: MBPP is a collection of 974 Python programming problems, designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and test cases.

**Why it's suitable for GRPO-ToT**:
- Simpler than HumanEval but still requires reasoning
- Has verifiable solutions through test cases
- Good for demonstrating basic reasoning capabilities
- Diverse problem types within a single domain

**Source**: [MBPP on HuggingFace](https://huggingface.co/datasets/mbpp)

## 6. BIG-Bench Hard

**Description**: BIG-Bench Hard is a subset of the BIG-Bench benchmark, focusing on the most challenging tasks. It includes problems across various domains including reasoning, mathematics, and common sense.

**Why it's suitable for GRPO-ToT**:
- Contains diverse reasoning tasks
- Many tasks have verifiable answers
- Challenging problems that benefit from tree exploration
- Well-established benchmark for advanced reasoning

**Source**: [BIG-Bench Hard](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks)

## Selection for Our Demonstration

For our demonstration, we will focus on the following datasets:

1. **GSM8K**: As a primary dataset for mathematical reasoning
2. **HumanEval**: For demonstrating code generation capabilities
3. **MBPP**: As a supplementary coding dataset with simpler problems

These datasets are chosen because:
1. They have verifiable answers, making them suitable for RLVR
2. They require multi-step reasoning, showcasing ToT's strengths
3. They are widely recognized benchmarks in the LLM community
4. They represent different types of reasoning (mathematical and algorithmic)
5. They are relatively compact, making them suitable for demonstration purposes

In the next steps, we will prepare these datasets for use with our GRPO-ToT implementation.
